# Reflection on Building with an AI Assistant

Building the backend for the Congregate application was a unique and insightful experience, largely defined by my collaboration with the Gemini AI assistant. The process felt less like using a tool and more like a high-speed pair-programming session with a partner who had encyclopedic knowledge but lacked real-world context. This dynamic profoundly impacted the build process, revealing both the incredible power and the distinct limitations of AI-driven development.

What worked exceptionally well was the speed of initial generation and scaffolding. When I needed to replace Prisma with Sequelize, the AI generated the necessary models, migrations, and configuration files in seconds. A task that would have taken me significant time in reading documentation and ensuring correct syntax was completed almost instantly. Similarly, when I requested a full JWT authentication setup with Passport.js, the AI produced all the required files—the strategy configuration, the routes, the middleware—in a single, coherent pass. For building out standard, well-documented features, the AI was an unparalleled accelerator. It took my high-level requirements and translated them into functional, boilerplate-heavy code, allowing me to focus on the larger architectural picture.

However, the process quickly highlighted the AI's limitations, particularly when things went wrong. The most challenging phase of our collaboration was the extensive debugging required to get the generated code to run. We encountered a cascade of errors, starting with module compatibility issues (ESM vs. CommonJS). This is where the AI's lack of environmental context became a significant hurdle. It initially produced code that was syntactically correct but incompatible with my project's `"type": "module"` setting. It felt like my partner was coding in a vacuum, unaware of the specific runtime I was using. The AI couldn't run commands to see the errors itself, so it relied entirely on the error logs I provided. This led to a frustrating but educational back-and-forth where we tried solution after solution—renaming files to `.cjs`, using explicit command-line flags, and refactoring module imports.

This iterative debugging loop taught me a great deal about how to prompt and interact with the AI effectively. My initial prompts were broad, like "set up authentication." I learned that while this works for generation, debugging requires extreme precision. I had to become adept at providing the *exact* error message, the relevant code snippets, and the context of what I had just tried. The AI, in turn, was surprisingly good at using this information to form new hypotheses. When we faced a mysterious `403 Forbidden` error, it correctly deduced a likely CORS issue and later helped isolate the problem to my local environment by systematically commenting out code. It was a lesson in treating the AI not as a magic box but as a logical system that needs clear, structured input to function effectively.

Ultimately, the experience was overwhelmingly positive. The AI dramatically compressed the development timeline. The debugging phase, while challenging, forced me to understand the underlying technologies at a much deeper level. I wasn't just getting working code; I was getting a real-time lesson in module systems, database drivers, and server configuration. The AI handled the "what" and the "how," but I was responsible for the "why" and the "where." It was a powerful symbiosis. I learned that the true skill in using an AI assistant isn't just in writing the first prompt, but in the patient, precise, and iterative process of reviewing, debugging, and guiding it toward a final, working solution.
